{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_of_params=45597\n",
      "names retrieved\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, fan_in, fan_out, bias=True) -> None:\n",
    "        self.weights = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x) -> torch.Tensor:\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weights, self.bias]\n",
    "        \n",
    "        return [self.weights]\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x) -> torch.Tensor:\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class BatchNormal1D:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdim=True)\n",
    "            xvar = x.var(dim, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self, vocab_size: int, no_of_embeddings: int) -> None:\n",
    "        self.weights = torch.randn((vocab_size, no_of_embeddings), generator=g)\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        # x should have a shape of (batch_size,)\n",
    "        self.out = self.weights[x]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "    \n",
    "class FlattenConsecutive:\n",
    "\n",
    "    def __init__(self, n) -> None:\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        B, T, C = x.shape\n",
    "        self.out: torch.Tensor = x.view((B, T//self.n, C*self.n))\n",
    "        if self.out.shape[1] == 1:\n",
    "            self.out = self.out.squeeze(1)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers: List) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return x\n",
    "    \n",
    "n_embed = 10\n",
    "n_hidden = 100\n",
    "block_size = 8\n",
    "\n",
    "def get_names() -> List[str]:\n",
    "    names = []\n",
    "    with open('../names.txt') as f:\n",
    "        # names = list(map(lambda x: x[:-1], (next(f) for _ in range(50))))\n",
    "        names = f.read().splitlines()\n",
    "    return names\n",
    "\n",
    "vocab_size = 27\n",
    "\n",
    "model: Sequential = Sequential([\n",
    "    Embedding(vocab_size, n_embed),                                                                    # 8 x 10\n",
    "    FlattenConsecutive(2), Linear(n_embed * 2, n_hidden, bias=False), BatchNormal1D(n_hidden), Tanh(), # 4 x 20\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNormal1D(n_hidden), Tanh(),    # 2 x 40\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNormal1D(n_hidden), Tanh(),    # 1 x 80\n",
    "    Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-3].gamma *= 0.1\n",
    "    for layer in model.layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weights *= 1.0\n",
    "\n",
    "parameters: List[torch.Tensor] = [p for layer in model.layers for p in layer.parameters()]\n",
    "no_of_params = sum(p.nelement() for p in parameters)\n",
    "print(f\"{no_of_params=}\")\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# building stoi\n",
    "def get_stoi() -> Dict[str, int]:\n",
    "    stoi = {c: i+1 for i, c in enumerate(string.ascii_lowercase)}\n",
    "    stoi['.'] = 0\n",
    "    return stoi\n",
    "\n",
    "\n",
    "# bulding itos\n",
    "def get_itos() -> List[str]:\n",
    "    itos = ['.'] + list(string.ascii_lowercase)\n",
    "    return itos\n",
    "\n",
    "stoi = get_stoi()\n",
    "itos = get_itos()\n",
    "def get_xs_ys_from_name(name: str, block_size: int) -> Tuple[List[List[int]], List[int]]:\n",
    "\n",
    "    xs, ys = [], []\n",
    "    x = [0] * block_size\n",
    "    for i in range(len(name)):\n",
    "        c1 = name[i]\n",
    "        c2 = name[i + 1] if i < len(name) - 1 else '.'\n",
    "        first = stoi[c1]\n",
    "        second = stoi[c2]\n",
    "\n",
    "        x.append(first)\n",
    "        x = x[1:]\n",
    "\n",
    "        xs.append(x.copy())\n",
    "        ys.append(second)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "def build_dataset(names: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for name in names:\n",
    "        x, y = get_xs_ys_from_name(name, block_size)\n",
    "        xs.extend(x)\n",
    "        ys.extend(y)\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "\n",
    "names = get_names()\n",
    "print(\"names retrieved\")\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "Xtr, Ytr = build_dataset(names[:n1])\n",
    "Xval, Yval = build_dataset(names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(names[n2:])\n",
    "\n",
    "# training\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb = Xtr[ix]\n",
    "x: torch.Tensor = model(Xb)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3217\n",
      "  10000/ 200000: 1.8252\n",
      "  20000/ 200000: 1.7584\n",
      "  30000/ 200000: 2.2835\n",
      "  40000/ 200000: 1.9354\n",
      "  50000/ 200000: 2.2560\n",
      "  60000/ 200000: 1.7906\n",
      "  70000/ 200000: 1.7284\n",
      "  80000/ 200000: 1.9224\n",
      "  90000/ 200000: 1.6209\n",
      " 100000/ 200000: 1.3516\n",
      " 110000/ 200000: 2.0380\n",
      " 120000/ 200000: 1.3887\n",
      " 130000/ 200000: 1.7233\n",
      " 140000/ 200000: 1.9910\n",
      " 150000/ 200000: 1.5905\n",
      " 160000/ 200000: 1.8258\n",
      " 170000/ 200000: 1.9257\n",
      " 180000/ 200000: 1.8604\n",
      " 190000/ 200000: 2.2050\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    # generating new batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    # forward\n",
    "    x: torch.Tensor = model(Xb)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "    # backward\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "         print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.6902440786361694\n",
      "val 1.8543689250946045\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xval, Yval),\n",
    "    'test': (Xtest, Ytest),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
